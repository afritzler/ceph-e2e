name: Run e2e tests

on:
  pull_request:
    types: [labeled, synchronize, reopened]
    paths-ignore:
      - 'docs/**'

jobs:
  e2e-tests:
    runs-on: ubuntu-latest
    if: contains(github.event.pull_request.labels.*.name, 'e2e-tests')

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - uses: actions/setup-go@v4
        with:
          go-version-file: 'go.mod'

      - uses: azure/setup-kubectl@v3

      - uses: azure/setup-helm@v3
        with:
          version: 'v3.12.2'

      - name: Update APT and install ceph libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcephfs-dev librbd-dev librados-dev

      - name: Start minikube
        uses: medyagh/setup-minikube@latest
        id: minikube
        with:
          cpus: 2
          memory: 4096m
          cni: bridge
          start-args: '--disk-size=40g --extra-disks 1'

      - name: Add Rook repository
        run: helm repo add rook-release https://charts.rook.io/release

      - name: Install Rook Operator
        run: helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph --set csi.enableRbdDriver=false --set csi.enableCephfsDriver=false

      - name: Verify Rook CRDs installation
        run: |
          end=$((SECONDS+20))  # 20 seconds timeout
          while [ $SECONDS -lt $end ]; do
            if kubectl get crds | grep -q 'cephclusters.ceph.rook.io'; then
              echo "Rook CRDs have been installed!"
              exit 0
            fi
            echo "Waiting for Rook CRDs to be installed..."
            sleep 10
          done
          
          echo "Timeout waiting for Rook CRDs!"
          exit 1

      - name: Apply CephCluster configuration
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: rook-config-override
            namespace: rook-ceph
          data:
            config: |
              [global]
              osd_pool_default_size = 1
              mon_warn_on_pool_no_redundancy = false
              bdev_flock_retry = 20
              bluefs_buffered_io = false
              mon_data_avail_warn = 10
          ---
          apiVersion: ceph.rook.io/v1
          kind: CephCluster
          metadata:
            name: my-cluster
            namespace: rook-ceph
          spec:
            cephVersion:
              allowUnsupported: true
              image: quay.io/ceph/ceph:v17
            cleanupPolicy:
              sanitizeDisks: {}
            crashCollector:
              disable: true
            dashboard:
              enabled: true
            dataDirHostPath: /var/lib/rook
            disruptionManagement:
              managePodBudgets: true
            external: {}
            healthCheck:
              daemonHealth:
                mon:
                  interval: 45s
                  timeout: 600s
                osd: {}
                status: {}
            logCollector: {}
            mgr:
              allowMultiplePerNode: true
              count: 1
            mon:
              allowMultiplePerNode: true
              count: 1
            monitoring: {}
            network:
              hostNetwork: true
            priorityClassNames:
              all: system-node-critical
              mgr: system-cluster-critical
            security:
              kms: {}
            storage:
              useAllDevices: true
              useAllNodes: true
          ---
          apiVersion: ceph.rook.io/v1
          kind: CephBlockPool
          metadata:
            name: cephlet-pool
            namespace: rook-ceph
          spec:
            name: .mgr
            erasureCoded:
              codingChunks: 0
              dataChunks: 0
            replicated:
              size: 1
              requireSafeReplicaSize: false
          ---
          apiVersion: ceph.rook.io/v1
          kind: CephClient
          metadata:
            name: cephlet-pool
            namespace: rook-ceph
          spec:
            caps:
              mgr: profile rbd pool=cephlet-pool
              mon: profile rbd
              osd: profile rbd pool=cephlet-pool
          EOF

      - name: Wait for CephCluster to be healthy or warning
        run: |
          echo "Waiting for CephCluster to be healthy or in warning status..."
          while true; do
          kubectl -n rook-ceph get pods 
          HEALTH_STATUS=$(kubectl -n rook-ceph get cephcluster -o jsonpath='{.items[0].status.ceph.health}')
          echo "CephCluster health status: $HEALTH_STATUS"
          if [[ "$HEALTH_STATUS" == "HEALTH_OK" || "$HEALTH_STATUS" == "HEALTH_WARN" ]]; then
            kubectl -n rook-ceph get pods
            kubectl -n rook-ceph logs -l app=rook-ceph-osd-prepare
            break
          fi
          sleep 30
          done
          echo "CephCluster is in acceptable state!"

      - name: Set Environment Variables
        run: |
          echo "CEPH_USERNAME=admin" >> $GITHUB_ENV
          echo "CEPH_POOLNAME=cephlet-pool" >> $GITHUB_ENV
          echo "CEPH_CLIENTNAME=client.cephlet-pool" >> $GITHUB_ENV
          keyring=$(kubectl -n rook-ceph get secret rook-ceph-admin-keyring -o jsonpath={.data.keyring})
          echo "CEPH_KEY=$keyring" >> $GITHUB_ENV
          mon=$(kubectl -n rook-ceph get cm rook-ceph-mon-endpoints -o jsonpath={.data.data})
          echo "CEPH_MONITORS=$(echo $mon | sed 's/^[^0-9[]*//')" >> $GITHUB_ENV

      - name: Run tests
        run: E2E_TESTS=true CGO=1 go test ./...
