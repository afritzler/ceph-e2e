name: Run e2e tests

on:
  pull_request:
    types: [labeled, synchronize, reopened]
    paths-ignore:
      - 'docs/**'

jobs:
  e2e-tests:
    runs-on: ubuntu-latest
    if: contains(github.event.pull_request.labels.*.name, 'e2e-tests')

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - uses: actions/setup-go@v4
        with:
          go-version-file: 'go.mod'

      - uses: azure/setup-kubectl@v3

      - uses: azure/setup-helm@v3
        with:
          version: 'v3.12.2'

      - name: Update APT and install ceph libs
        run: |
          sudo apt-get update
          sudo apt-get install -y libcephfs-dev librbd-dev librados-dev

      - name: Start minikube
        uses: medyagh/setup-minikube@latest
        id: minikube
        with:
          cpus: 2
          memory: 6000m
          addons: storage-provisioner

      - name: Add Rook repository
        run: helm repo add rook-release https://charts.rook.io/release

      - name: Install Rook Operator
        run: helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph --set csi.enableRbdDriver=false --set csi.enableCephfsDriver=false

      - name: Verify Rook CRDs installation
        run: |
          end=$((SECONDS+20))  # 20 seconds timeout
          while [ $SECONDS -lt $end ]; do
            if kubectl get crds | grep -q 'cephclusters.ceph.rook.io'; then
              echo "Rook CRDs have been installed!"
              exit 0
            fi
            echo "Waiting for Rook CRDs to be installed..."
            sleep 10
          done
          
          echo "Timeout waiting for Rook CRDs!"
          exit 1

      - name: Apply CephCluster configuration
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: rook-config-override
            namespace: rook-ceph
          data:
            config: |
              [global]
              osd_pool_default_size = 1
              mon_warn_on_pool_no_redundancy = false
              bdev_flock_retry = 20
              bluefs_buffered_io = false
              mon_data_avail_warn = 10
          ---
          kind: CephCluster
          metadata:
            name: rook-ceph
            namespace: rook-ceph # namespace:cluster
          spec:
            dataDirHostPath: /var/lib/rook
            mon:
              count: 1
              allowMultiplePerNode: false
              volumeClaimTemplate:
                spec:
                  storageClassName: default
                  resources:
                    requests:
                      storage: 10Gi
            cephVersion:
              image: quay.io/ceph/ceph:v17.2.6
              allowUnsupported: false
            skipUpgradeChecks: false
            continueUpgradeAfterChecksEvenIfNotHealthy: false
            mgr:
              count: 1
              modules:
                - name: pg_autoscaler
                  enabled: true
            dashboard:
              enabled: true
              ssl: true
            crashCollector:
              disable: false
            logCollector:
              enabled: true
              periodicity: daily # one of: hourly, daily, weekly, monthly
              maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.
            storage:
              storageClassDeviceSets:
                - name: set1
                  count: 1
                  portable: false
                  tuneDeviceClass: false
                  tuneFastDeviceClass: false
                  encrypted: false
                  placement:
                    topologySpreadConstraints:
                      - maxSkew: 1
                        topologyKey: kubernetes.io/hostname
                        whenUnsatisfiable: ScheduleAnyway
                        labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd
                  preparePlacement:
                    podAntiAffinity:
                      preferredDuringSchedulingIgnoredDuringExecution:
                        - weight: 100
                          podAffinityTerm:
                            labelSelector:
                              matchExpressions:
                                - key: app
                                  operator: In
                                  values:
                                    - rook-ceph-osd
                                - key: app
                                  operator: In
                                  values:
                                    - rook-ceph-osd-prepare
                            topologyKey: kubernetes.io/hostname
                    topologySpreadConstraints:
                      - maxSkew: 1
                        topologyKey: topology.kubernetes.io/zone
                        whenUnsatisfiable: DoNotSchedule
                        labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd-prepare
                  resources:
                  # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
                  #   limits:
                  #     cpu: "500m"
                  #     memory: "4Gi"
                  #   requests:
                  #     cpu: "500m"
                  #     memory: "4Gi"
                  volumeClaimTemplates:
                    - metadata:
                        name: data
                      spec:
                        resources:
                          requests:
                            storage: 10Gi
                        storageClassName: default
                        volumeMode: Block
                        accessModes:
                          - ReadWriteOnce
              onlyApplyOSDPlacement: false
            resources:
            #  prepareosd:
            #    limits:
            #      cpu: "200m"
            #      memory: "200Mi"
            #   requests:
            #      cpu: "200m"
            #      memory: "200Mi"
            priorityClassNames:
              mon: system-node-critical
              osd: system-node-critical
              mgr: system-cluster-critical
            disruptionManagement:
              managePodBudgets: true
              osdMaintenanceTimeout: 30
              pgHealthCheckTimeout: 0
          ---
          apiVersion: ceph.rook.io/v1
          kind: CephBlockPool
          metadata:
            name: cephlet-pool
            namespace: rook-ceph
          spec:
            name: .mgr
            erasureCoded:
              codingChunks: 0
              dataChunks: 0
            replicated:
              size: 1
              requireSafeReplicaSize: false
          ---
          apiVersion: ceph.rook.io/v1
          kind: CephClient
          metadata:
            name: cephlet-pool
            namespace: rook-ceph
          spec:
            caps:
              mgr: profile rbd pool=cephlet-pool
              mon: profile rbd
              osd: profile rbd pool=cephlet-pool
          EOF

      - name: Wait for CephCluster to be healthy or warning
        run: |
          echo "Waiting for CephCluster to be healthy or in warning status..."
          while true; do
          kubectl -n rook-ceph get pods 
          HEALTH_STATUS=$(kubectl -n rook-ceph get cephcluster -o jsonpath='{.items[0].status.ceph.health}')
          echo "CephCluster health status: $HEALTH_STATUS"
          if [[ "$HEALTH_STATUS" == "HEALTH_OK" || "$HEALTH_STATUS" == "HEALTH_WARN" ]]; then
           break
          fi
          sleep 30
          done
          echo "CephCluster is in acceptable state!"

      - name: Set Environment Variables
        run: |
          echo "CEPH_USERNAME=admin" >> $GITHUB_ENV
          echo "CEPH_POOLNAME=cephlet-pool" >> $GITHUB_ENV
          echo "CEPH_CLIENTNAME=client.cephlet-pool" >> $GITHUB_ENV
          keyring=$(kubectl -n rook-ceph get secret rook-ceph-admin-keyring -o jsonpath={.data.keyring})
          echo "CEPH_KEY=$keyring" >> $GITHUB_ENV
          echo "CEPH_KEY=$keyring"
          mon=$(kubectl -n rook-ceph get cm rook-ceph-mon-endpoints -o jsonpath={.data.data})
          echo "CEPH_MONITORS=$(echo $mon | sed 's/^[^0-9[]*//')" >> $GITHUB_ENV
          echo "CEPH_MONITORS=$(echo $mon | sed 's/^[^0-9[]*//')"

      - name: Run tests
        run: E2E_TEST=true CGO=1 go test ./...
